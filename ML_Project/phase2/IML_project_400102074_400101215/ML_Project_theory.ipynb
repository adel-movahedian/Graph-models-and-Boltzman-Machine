{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0 size=6>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=250 height=250> <br>\n",
        "<font color=0F5298 size=7>\n",
        "Introduction to Machine Learning <br>\n",
        "<font color=2565AE size=5>\n",
        "Dr. Mohammad Hossein Yassaee<br>\n",
        "<font color=\"green\" size=5>\n",
        "Adel Movahedian -\n",
        "Mehdi zolfaghari<br>\n",
        "<font color=2565AE size=5>\n",
        "winter 2024<br>\n"
      ],
      "metadata": {
        "id": "wtbCo4qlkAUq"
      },
      "id": "wtbCo4qlkAUq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1:<br>\n",
        " Using the MNIST dataset and employing a Safe Boltzmann Machine as a feature extractor, perform a classification task and compare its performance with a standard Boltzmann Machine."
      ],
      "metadata": {
        "id": "2Kq9L3nPkM49"
      },
      "id": "2Kq9L3nPkM49"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def preprocess_mnist(threshold=128):\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "    train_images = train_images.reshape(-1, 28 * 28) / 255.0\n",
        "    test_images = test_images.reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "    train_images = (train_images > threshold / 255.0).astype(np.float32)\n",
        "    test_images = (test_images > threshold / 255.0).astype(np.float32)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "def show_samples(images, labels, num_samples=10):\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(f\"Label: {labels[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "train_images, train_labels, test_images, test_labels = preprocess_mnist()\n",
        "\n",
        "show_samples(train_images, train_labels, num_samples=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "nBz3I3BHkbN4",
        "outputId": "ad41fc27-2714-4930-ce1c-b8bc936fec67"
      },
      "id": "nBz3I3BHkbN4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "##################\n",
        "#      RBM       #\n",
        "##################\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, num_visible, num_hidden, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "        self.lr = lr\n",
        "        # Initialize weights using Xavier to avoid possible gradient exploding\n",
        "        self.W = nn.Parameter(torch.empty(num_visible, num_hidden))\n",
        "        nn.init.xavier_normal_(self.W, gain=0.01)\n",
        "        self.v_bias = nn.Parameter(torch.zeros(num_visible))\n",
        "        self.h_bias = nn.Parameter(torch.zeros(num_hidden))\n",
        "\n",
        "    def sample_hidden(self, v):\n",
        "        if not isinstance(v, torch.Tensor):\n",
        "          v = torch.tensor(v, dtype=torch.float32)\n",
        "        activation = v @ self.W + self.h_bias  #  @ : matrix multiplication\n",
        "        prob_h = torch.sigmoid(activation)\n",
        "        sample_h = torch.bernoulli(prob_h)\n",
        "        return prob_h, sample_h\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        if not isinstance(h, torch.Tensor):\n",
        "          h = torch.tensor(h, dtype=torch.float32)\n",
        "        activation = h @ self.W.t() + self.v_bias\n",
        "        prob_v = torch.sigmoid(activation)\n",
        "        sample_v = torch.bernoulli(prob_v)\n",
        "        return prob_v, sample_v\n",
        "\n",
        "    def contrastive_divergence(self, v0):\n",
        "        if not isinstance(v0, torch.Tensor):\n",
        "          v0 = torch.tensor(v0, dtype=torch.float32)\n",
        "        p_h0, h0 = self.sample_hidden(v0) # Sample hidden units given the initial visible states\n",
        "        p_v, v1 = self.sample_visible(h0) # Reconstruct the visible states from hidden samples\n",
        "        p_h1, _ = self.sample_hidden(v1) # Sample hidden units again from the reconstructed visibles\n",
        "\n",
        "        loss = torch.mean((v0 - v1) ** 2) # MSE\n",
        "\n",
        "        batch_size = v0.size(0)\n",
        "        with torch.no_grad():\n",
        "            self.W += self.lr * ((v0.t() @ p_h0) - (v1.t() @ p_h1)) / batch_size\n",
        "            self.v_bias += self.lr * torch.mean(v0 - v1, dim=0)\n",
        "            self.h_bias += self.lr * torch.mean(p_h0 - p_h1, dim=0)\n",
        "        return loss\n",
        "\n",
        "    def train_rbm(self, train_images, epochs=10, batch_size=20):\n",
        "        num_samples = len(train_images)\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_loss = 0.0\n",
        "            for start in range(0, num_samples, batch_size):\n",
        "                batch = train_images[start:start + batch_size]\n",
        "                loss = self.contrastive_divergence(batch)\n",
        "                epoch_loss += loss.item()\n",
        "            avg_loss = epoch_loss / (num_samples / batch_size)\n",
        "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "        print(\"RBM is trained.\")\n",
        "\n",
        "    def free_energy(self, v): # with this function we measure the quality of given visible state.\n",
        "        v_transpose_bias = -v @ self.v_bias\n",
        "        # instead of (log(1 + exp(x))) we use torch.log1p for numerical stability\n",
        "        hidden_term = -torch.sum(torch.log1p(torch.exp(v @ self.W + self.h_bias)), dim=1)\n",
        "        return v_transpose_bias + hidden_term\n",
        "\n",
        "##################\n",
        "#     ElGamal    #\n",
        "##################\n",
        "class ElGamal:\n",
        "    def __init__(self, prime=99991, generator=2):\n",
        "        self.prime = prime\n",
        "        self.generator = generator\n",
        "        self.private_key = random.randint(1, self.prime - 2)\n",
        "        self.public_key = pow(self.generator, self.private_key, self.prime) # generator to the power of private_key, modulus prime\n",
        "\n",
        "    def encrypt(self, message):\n",
        "        if isinstance(message, float):\n",
        "            message = int(message * 10**6) # if the message is fload the message scales into int.\n",
        "        message = message % self.prime  # Ensure the message is within the valid range.\n",
        "        ephemeral_key = random.randint(1, self.prime - 2) # Select a random ephemeral key.\n",
        "        first_component = pow(self.generator, ephemeral_key, self.prime)\n",
        "        second_component = (message * pow(self.public_key, ephemeral_key, self.prime)) % self.prime\n",
        "        return first_component, second_component\n",
        "\n",
        "    def decrypt(self, c1, c2):\n",
        "        shared_secret = pow(c1, self.private_key, self.prime)\n",
        "        # Calculate its modular inverse using Fermat's little theorem.\n",
        "        inverse_secret = pow(shared_secret, self.prime - 2, self.prime)\n",
        "        decrypted_value = (c2 * inverse_secret) % self.prime\n",
        "        if decrypted_value > self.prime // 2:\n",
        "            decrypted_value -= self.prime\n",
        "        return decrypted_value / 10**6\n",
        "\n",
        "##################\n",
        "#    SecureRBM   #\n",
        "##################\n",
        "class SecureRBM(nn.Module):\n",
        "    def __init__(self, num_visible, num_hidden, lr=0.01, encryption_scheme=None):\n",
        "        super().__init__()\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "        self.lr = lr\n",
        "        self.crypto = encryption_scheme or ElGamal()\n",
        "        self.W = nn.Parameter(torch.randn(num_visible, num_hidden) * 0.01)\n",
        "        self.v_bias = nn.Parameter(torch.zeros(num_visible))\n",
        "        self.h_bias = nn.Parameter(torch.zeros(num_hidden))\n",
        "        self.W_encrypted, self.v_bias_encrypted, self.h_bias_encrypted = [], [], []\n",
        "        self.encrypt_parameters()\n",
        "\n",
        "    def encrypt_parameters(self):\n",
        "        # Encrypts the model parameters and stores them separately.\n",
        "        with torch.no_grad():\n",
        "            self.W_encrypted = [[self.crypto.encrypt(float(w)) for w in row] for row in self.W]\n",
        "            self.v_bias_encrypted = [self.crypto.encrypt(float(v)) for v in self.v_bias]\n",
        "            self.h_bias_encrypted = [self.crypto.encrypt(float(h)) for h in self.h_bias]\n",
        "\n",
        "    def decrypt_parameters(self):\n",
        "        # Decrypts and returns the model parameters as tensors.\n",
        "        with torch.no_grad():\n",
        "            W_decrypted = torch.tensor([[self.crypto.decrypt(*w) for w in row] for row in self.W_encrypted], dtype=torch.float32)\n",
        "            v_bias_decrypted = torch.tensor([self.crypto.decrypt(*v) for v in self.v_bias_encrypted], dtype=torch.float32)\n",
        "            h_bias_decrypted = torch.tensor([self.crypto.decrypt(*h) for h in self.h_bias_encrypted], dtype=torch.float32)\n",
        "        return W_decrypted, v_bias_decrypted, h_bias_decrypted\n",
        "\n",
        "    def sample_hidden(self, v):\n",
        "        if not isinstance(v, torch.Tensor):\n",
        "          v = torch.tensor(v, dtype=torch.float32)\n",
        "        prob_h = torch.sigmoid(v @ self.W + self.h_bias)\n",
        "        sample_h = torch.bernoulli(prob_h)\n",
        "        return prob_h, sample_h\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        if not isinstance(h, torch.Tensor):\n",
        "          h = torch.tensor(h, dtype=torch.float32)\n",
        "        prob_v = torch.sigmoid(h @ self.W.t() + self.v_bias)\n",
        "        sample_v = torch.bernoulli(prob_v)\n",
        "        return prob_v, sample_v\n",
        "\n",
        "    def contrastive_divergence(self, v0):\n",
        "        if not isinstance(v0, torch.Tensor):\n",
        "          v0 = torch.tensor(v0, dtype=torch.float32)\n",
        "        _, h0 = self.sample_hidden(v0)\n",
        "        _, v_k = self.sample_visible(h0)\n",
        "        _, h_k = self.sample_hidden(v_k)\n",
        "        loss = torch.mean((v0 - v_k) ** 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.W += self.lr * ((v0.t() @ h0) - (v_k.t() @ h_k)) / v0.size(0)\n",
        "            self.v_bias += self.lr * torch.mean(v0 - v_k, dim=0)\n",
        "            self.h_bias += self.lr * torch.mean(h0 - h_k, dim=0)\n",
        "        return loss\n",
        "\n",
        "    def train_rbm(self, data, epochs=10, batch_size=20):\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            self.encrypt_parameters()\n",
        "            total_loss = 0.0\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                batch = data[i: i + batch_size]\n",
        "                total_loss += self.contrastive_divergence(batch).item()\n",
        "\n",
        "            avg_loss = total_loss / (len(data) / batch_size)\n",
        "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "            self.decrypt_parameters()\n",
        "        print(\"Secure RBM is trained\")\n",
        "\n",
        "\n",
        "\n",
        "def get_hidden_features(model, data):\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        _, hidden_layer = model.sample_hidden(data_tensor)\n",
        "    return hidden_layer.numpy()\n",
        "\n",
        "def evaluate_classifiers(rbm_model, secure_rbm_model, train_images, train_labels , test_images, test_labels):\n",
        "\n",
        "    # Extract features from simple RBM :\n",
        "    features_train_rbm = get_hidden_features(rbm_model, train_images)\n",
        "    features_test_rbm = get_hidden_features(rbm_model, test_images)\n",
        "    # Extract features from Secure RBM :\n",
        "    features_train_secure = get_hidden_features(secure_rbm_model, train_images)\n",
        "    features_test_secure = get_hidden_features(secure_rbm_model, test_images)\n",
        "\n",
        "    # Initialise and train logistic regression for RBM features\n",
        "    classifier_rbm = LogisticRegression(max_iter=10000)\n",
        "    classifier_rbm.fit(features_train_rbm, train_labels)\n",
        "    predictions_rbm = classifier_rbm.predict(features_test_rbm)\n",
        "    accuracy_rbm = accuracy_score(test_labels, predictions_rbm)\n",
        "\n",
        "    # Initialise and train logistic regression for SecureRBM features\n",
        "    classifier_secure = LogisticRegression(max_iter=10000)\n",
        "    classifier_secure.fit(features_train_secure, train_labels)\n",
        "    predictions_secure = classifier_secure.predict(features_test_secure)\n",
        "    accuracy_secure = accuracy_score(test_labels, predictions_secure)\n",
        "\n",
        "    print(\"RBM Feature Accuracy: {:.4f}\".format(accuracy_rbm))\n",
        "    print(\"SecureRBM Feature Accuracy: {:.4f}\".format(accuracy_secure))\n",
        "\n",
        "    return accuracy_rbm, accuracy_secure\n"
      ],
      "metadata": {
        "id": "2Wns_uO6kkou"
      },
      "id": "2Wns_uO6kkou",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rbm = RBM(num_visible=784, num_hidden=256, lr=0.01)\n",
        "secure_rbm = SecureRBM(num_visible=784, num_hidden=256, lr=0.01, encryption_scheme=ElGamal())\n",
        "rbm.train_rbm(train_images, epochs=20)\n",
        "secure_rbm.train_rbm(train_images, epochs=20)\n",
        "rbm_acc, secure_acc= evaluate_classifiers(rbm, secure_rbm, train_images, train_labels, test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZkvHK6qlS-V",
        "outputId": "def67160-c10e-4a38-8fb9-ff2ac62c6758"
      },
      "id": "QZkvHK6qlS-V",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.1325\n",
            "Epoch 2, Average Loss: 0.0875\n",
            "Epoch 3, Average Loss: 0.0741\n",
            "Epoch 4, Average Loss: 0.0669\n",
            "Epoch 5, Average Loss: 0.0619\n",
            "Epoch 6, Average Loss: 0.0584\n",
            "Epoch 7, Average Loss: 0.0555\n",
            "Epoch 8, Average Loss: 0.0532\n",
            "Epoch 9, Average Loss: 0.0513\n",
            "Epoch 10, Average Loss: 0.0497\n",
            "Epoch 11, Average Loss: 0.0482\n",
            "Epoch 12, Average Loss: 0.0469\n",
            "Epoch 13, Average Loss: 0.0458\n",
            "Epoch 14, Average Loss: 0.0447\n",
            "Epoch 15, Average Loss: 0.0438\n",
            "Epoch 16, Average Loss: 0.0430\n",
            "Epoch 17, Average Loss: 0.0422\n",
            "Epoch 18, Average Loss: 0.0416\n",
            "Epoch 19, Average Loss: 0.0410\n",
            "Epoch 20, Average Loss: 0.0404\n",
            "RBM is trained.\n",
            "Epoch 1, Average Loss: 0.1149\n",
            "Epoch 2, Average Loss: 0.0792\n",
            "Epoch 3, Average Loss: 0.0692\n",
            "Epoch 4, Average Loss: 0.0632\n",
            "Epoch 5, Average Loss: 0.0593\n",
            "Epoch 6, Average Loss: 0.0563\n",
            "Epoch 7, Average Loss: 0.0539\n",
            "Epoch 8, Average Loss: 0.0519\n",
            "Epoch 9, Average Loss: 0.0502\n",
            "Epoch 10, Average Loss: 0.0488\n",
            "Epoch 11, Average Loss: 0.0475\n",
            "Epoch 12, Average Loss: 0.0464\n",
            "Epoch 13, Average Loss: 0.0453\n",
            "Epoch 14, Average Loss: 0.0444\n",
            "Epoch 15, Average Loss: 0.0436\n",
            "Epoch 16, Average Loss: 0.0428\n",
            "Epoch 17, Average Loss: 0.0421\n",
            "Epoch 18, Average Loss: 0.0415\n",
            "Epoch 19, Average Loss: 0.0410\n",
            "Epoch 20, Average Loss: 0.0404\n",
            "Secure RBM is trained\n",
            "RBM Feature Accuracy: 0.9226\n",
            "SecureRBM Feature Accuracy: 0.9207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"green\">Secure RBM provides privacy-preserving learning but may slightly reduce classification accuracy compared to standard RBM because of encryption and decryption we did."
      ],
      "metadata": {
        "id": "6R7F1VmrfPfa"
      },
      "id": "6R7F1VmrfPfa"
    },
    {
      "cell_type": "markdown",
      "id": "d43b857c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-02T09:27:48.502625Z",
          "start_time": "2025-02-02T09:27:48.478686Z"
        },
        "id": "d43b857c"
      },
      "source": [
        "# Question 2:\n",
        "using the p(x,y) as loss function, implimenting the RBM as bellow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2d66a6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-02T14:23:57.922928Z",
          "start_time": "2025-02-02T14:10:06.574634Z"
        },
        "id": "7d2d66a6",
        "outputId": "0cd1a38b-ecc4-4283-a10c-ef7c10dd5c78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zoli\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 91.86%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_classes):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Parameters\n",
        "        self.W = np.random.randn(n_hidden, n_visible) * 0.01  # Weights (hidden x visible)\n",
        "        self.U = np.random.randn(n_hidden, n_classes) * 0.01  # Weights (hidden x class)\n",
        "        self.b = np.zeros(n_visible)  # Visible bias (input)\n",
        "        self.c = np.zeros(n_hidden)   # Hidden bias\n",
        "        self.d = np.zeros(n_classes)  # Class bias\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def sample_hidden(self, x, y):\n",
        "        \"\"\"Sample hidden units given x and y.\"\"\"\n",
        "        activation = self.c + np.dot(self.W, x) + self.U[:, y]\n",
        "        p_h = self.sigmoid(activation)\n",
        "        return p_h, np.random.binomial(1, p_h)\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        \"\"\"Sample x and y given hidden units.\"\"\"\n",
        "        # Sample x (binary)\n",
        "        activation_x = self.b + np.dot(self.W.T, h)\n",
        "        p_x = self.sigmoid(activation_x)\n",
        "        x_recon = np.random.binomial(1, p_x)\n",
        "\n",
        "        # Sample y (softmax)\n",
        "        activation_y = self.d + np.dot(self.U.T, h)\n",
        "        p_y = self.softmax(activation_y)\n",
        "        y_recon = np.random.choice(self.n_classes, p=p_y)\n",
        "        return x_recon, y_recon\n",
        "    # attribute used to evaluate the model\n",
        "    def p_y_given_x(self, x):\n",
        "        \"\"\"Compute p(y|x) using exact inference.\"\"\"\n",
        "        # Shape: (n_hidden, n_classes)\n",
        "        activation = self.c[:, np.newaxis] + np.dot(self.W, x)[:, np.newaxis] + self.U\n",
        "        log_terms = np.log(1 + np.exp(activation))\n",
        "        sum_log_terms = np.sum(log_terms, axis=0)  # Sum over hidden units\n",
        "        logits = self.d + sum_log_terms\n",
        "        return self.softmax(logits)\n",
        "\n",
        "    def generative_gradient(self, x, y, n_gibbs=1):\n",
        "        \"\"\"Compute generative gradient using Contrastive Divergence (CD-k).\"\"\"\n",
        "        # Positive phase\n",
        "        p_h_pos, h_pos = self.sample_hidden(x, y)\n",
        "\n",
        "        # Negative phase (CD-1)\n",
        "        x_neg, y_neg = x.copy(), y\n",
        "        for _ in range(n_gibbs):\n",
        "            # Sample y_neg from p(y|h_pos)\n",
        "            p_y_neg = self.softmax(self.d + np.dot(self.U.T, h_pos))\n",
        "            y_neg = np.random.choice(self.n_classes, p=p_y_neg)\n",
        "            # Sample x_neg from p(x|h_pos)\n",
        "            p_x_neg = self.sigmoid(self.b + np.dot(self.W.T, h_pos))\n",
        "            x_neg = np.random.binomial(1, p_x_neg)\n",
        "            # Sample h_neg from p(h|x_neg, y_neg)\n",
        "            _, h_neg = self.sample_hidden(x_neg, y_neg)\n",
        "\n",
        "        # Compute gradients\n",
        "        #each parameter (theta) undergoes the positive and negative phase as shown in the given algorithm\n",
        "        #gradient of Lgenerative w.r.t W,U,b,c,d\n",
        "        grad_W = np.outer(h_pos, x) - np.outer(h_neg, x_neg)\n",
        "        grad_U = np.zeros_like(self.U)\n",
        "        grad_U[:, y] = h_pos\n",
        "        grad_U[:, y_neg] -= h_neg\n",
        "        grad_b = x - x_neg\n",
        "        grad_c = h_pos - h_neg\n",
        "        grad_d = np.zeros(self.n_classes)\n",
        "        grad_d[y] = 1\n",
        "        grad_d[y_neg] -= 1\n",
        "\n",
        "        return {'W': grad_W, 'U': grad_U, 'b': grad_b, 'c': grad_c, 'd': grad_d}\n",
        "    # updating parameters with learning rate lr and gradients calculated above\n",
        "    def update_parameters(self, grads, lr):\n",
        "        \"\"\"Update parameters with gradients.\"\"\"\n",
        "        self.W += lr * grads.get('W', 0)\n",
        "        self.U += lr * grads.get('U', 0)\n",
        "        self.b += lr * grads.get('b', 0)\n",
        "        self.c += lr * grads.get('c', 0)\n",
        "        self.d += lr * grads.get('d', 0)\n",
        "\n",
        "    def train(self, labeled_data, lr=0.01, epochs=10):\n",
        "        \"\"\"Training loop for RBM.\"\"\"\n",
        "        for _ in range(epochs):\n",
        "            np.random.shuffle(labeled_data)\n",
        "            for x, y in labeled_data:\n",
        "                gen_grads = self.generative_gradient(x, y)\n",
        "                self.update_parameters(gen_grads, lr)\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Binarize the data\n",
        "X_binarized = (X > 127).astype(int)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare labeled data as (input, target) pairs\n",
        "labeled_data = list(zip(X_train.values, y_train))\n",
        "\n",
        "# Initialize RBM\n",
        "n_visible = 784  # 28x28 pixels\n",
        "n_hidden = 200   # Number of hidden units\n",
        "n_classes = 10   # Digits 0-9\n",
        "rbm = RBM(n_visible, n_hidden, n_classes)\n",
        "\n",
        "# Train RBM\n",
        "rbm.train(labeled_data, lr=0.01, epochs=10)\n",
        "\n",
        "# Evaluate p(y|x) on the test set\n",
        "y_pred = []\n",
        "for x in X_test.values:\n",
        "    p_y = rbm.p_y_given_x(x)\n",
        "    y_pred.append(np.argmax(p_y))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430379f9",
      "metadata": {
        "id": "430379f9"
      },
      "source": [
        "# Question 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96127960",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-02T14:37:43.357528Z",
          "start_time": "2025-02-02T14:27:34.066431Z"
        },
        "id": "96127960",
        "outputId": "0cc8ca29-2236-4fee-8fbd-6b3781dbbf75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zoli\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 97.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class DRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_classes):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Parameters\n",
        "        self.W = np.random.randn(n_hidden, n_visible) * 0.01  # Weights (hidden x visible)\n",
        "        self.U = np.random.randn(n_hidden, n_classes) * 0.01  # Weights (hidden x class)\n",
        "        self.b = np.zeros(n_visible)  # Visible bias (input)\n",
        "        self.c = np.zeros(n_hidden)   # Hidden bias\n",
        "        self.d = np.zeros(n_classes)  # Class bias\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def sample_hidden(self, x, y):\n",
        "        \"\"\"Sample hidden units given x and y.\"\"\"\n",
        "        activation = self.c + np.dot(self.W, x) + self.U[:, y]\n",
        "        p_h = self.sigmoid(activation)\n",
        "        return p_h, np.random.binomial(1, p_h)\n",
        "\n",
        "    def p_y_given_x(self, x):\n",
        "        \"\"\"Compute p(y|x) using exact inference.\"\"\"\n",
        "        # Shape: (n_hidden, n_classes)\n",
        "        activation = self.c[:, np.newaxis] + np.dot(self.W, x)[:, np.newaxis] + self.U\n",
        "        log_terms = np.log(1 + np.exp(activation))\n",
        "        sum_log_terms = np.sum(log_terms, axis=0)  # Sum over hidden units\n",
        "        logits = self.d + sum_log_terms\n",
        "        return self.softmax(logits)\n",
        "\n",
        "    def discriminative_gradient(self, x, y):\n",
        "        \"\"\"Compute exact gradient for discriminative loss (log p(y|x)).\"\"\"\n",
        "        p_y = self.p_y_given_x(x)\n",
        "        o_matrix = self.c[:, np.newaxis] + np.dot(self.W, x)[:, np.newaxis] + self.U\n",
        "        sigmoid_o = self.sigmoid(o_matrix)\n",
        "\n",
        "        # Gradients of Ldiscriminative w.r.t c, W, U, d\n",
        "        positive_contrib = sigmoid_o[:, y]\n",
        "        negative_contrib = np.dot(sigmoid_o, p_y)\n",
        "\n",
        "        grad_c = positive_contrib - negative_contrib\n",
        "        grad_W = np.outer(grad_c, x)\n",
        "        grad_U = np.zeros_like(self.U)\n",
        "        grad_U[:, y] = positive_contrib\n",
        "        grad_U -= sigmoid_o * p_y[np.newaxis, :]\n",
        "\n",
        "        grad_d = np.zeros(self.n_classes)\n",
        "        grad_d[y] = 1\n",
        "        grad_d -= p_y\n",
        "\n",
        "        return {'W': grad_W, 'U': grad_U, 'c': grad_c, 'd': grad_d}\n",
        "\n",
        "    def update_parameters(self, grads, lr):\n",
        "        \"\"\"Update parameters with gradients.\"\"\"\n",
        "        self.W += lr * grads.get('W', 0)\n",
        "        self.U += lr * grads.get('U', 0)\n",
        "        self.b += lr * grads.get('b', 0)\n",
        "        self.c += lr * grads.get('c', 0)\n",
        "        self.d += lr * grads.get('d', 0)\n",
        "\n",
        "    def train(self, labeled_data, lr=0.01, epochs=10):\n",
        "        \"\"\"Training loop for DRBM.\"\"\"\n",
        "        for _ in range(epochs):\n",
        "            np.random.shuffle(labeled_data)\n",
        "            for x, y in labeled_data:\n",
        "                disc_grads = self.discriminative_gradient(x, y)\n",
        "                self.update_parameters(disc_grads, lr)\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Binarize the data\n",
        "X_binarized = (X > 127).astype(int)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare labeled data as (input, target) pairs\n",
        "labeled_data = list(zip(X_train.values, y_train))\n",
        "\n",
        "# Initialize DRBM\n",
        "n_visible = 784  # 28x28 pixels\n",
        "n_hidden = 200   # Number of hidden units\n",
        "n_classes = 10   # Digits 0-9\n",
        "drbm = DRBM(n_visible, n_hidden, n_classes)\n",
        "\n",
        "# Train DRBM\n",
        "drbm.train(labeled_data, lr=0.01, epochs=10)\n",
        "\n",
        "# Evaluate p(y|x) on the test set\n",
        "y_pred = []\n",
        "for x in X_test.values:\n",
        "    p_y = drbm.p_y_given_x(x)\n",
        "    y_pred.append(np.argmax(p_y))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6949ae4d",
      "metadata": {
        "id": "6949ae4d"
      },
      "source": [
        "# Question 4:\n",
        "in each epoch, we used a combination of lgen and ldisc in order to lower the summation with a factor alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c681598a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-02T15:02:22.448539Z",
          "start_time": "2025-02-02T14:37:43.359522Z"
        },
        "id": "c681598a",
        "outputId": "d189d8c6-4a8e-417f-9af8-5674a5bc7c42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zoli\\anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 95.90%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class HDRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_classes):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Parameters\n",
        "        self.W = np.random.randn(n_hidden, n_visible) * 0.01  # Weights (hidden x visible)\n",
        "        self.U = np.random.randn(n_hidden, n_classes) * 0.01  # Weights (hidden x class)\n",
        "        self.b = np.zeros(n_visible)  # Visible bias (input)\n",
        "        self.c = np.zeros(n_hidden)   # Hidden bias\n",
        "        self.d = np.zeros(n_classes)  # Class bias\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def sample_hidden(self, x, y):\n",
        "        \"\"\"Sample hidden units given x and y.\"\"\"\n",
        "        activation = self.c + np.dot(self.W, x) + self.U[:, y]\n",
        "        p_h = self.sigmoid(activation)\n",
        "        return p_h, np.random.binomial(1, p_h)\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        \"\"\"Sample x and y given hidden units.\"\"\"\n",
        "        # Sample x (binary)\n",
        "        activation_x = self.b + np.dot(self.W.T, h)\n",
        "        p_x = self.sigmoid(activation_x)\n",
        "        x_recon = np.random.binomial(1, p_x)\n",
        "\n",
        "        # Sample y (softmax)\n",
        "        activation_y = self.d + np.dot(self.U.T, h)\n",
        "        p_y = self.softmax(activation_y)\n",
        "        y_recon = np.random.choice(self.n_classes, p=p_y)\n",
        "        return x_recon, y_recon\n",
        "\n",
        "    def p_y_given_x(self, x):\n",
        "        \"\"\"Compute p(y|x) using exact inference.\"\"\"\n",
        "        # Shape: (n_hidden, n_classes)\n",
        "        activation = self.c[:, np.newaxis] + np.dot(self.W, x)[:, np.newaxis] + self.U\n",
        "        log_terms = np.log(1 + np.exp(activation))\n",
        "        sum_log_terms = np.sum(log_terms, axis=0)  # Sum over hidden units\n",
        "        logits = self.d + sum_log_terms\n",
        "        return self.softmax(logits)\n",
        "\n",
        "    def discriminative_gradient(self, x, y):\n",
        "        \"\"\"Compute exact gradient for discriminative loss (log p(y|x)).\"\"\"\n",
        "        p_y = self.p_y_given_x(x)\n",
        "        o_matrix = self.c[:, np.newaxis] + np.dot(self.W, x)[:, np.newaxis] + self.U\n",
        "        sigmoid_o = self.sigmoid(o_matrix)\n",
        "\n",
        "        # Gradients for c, W, U, d\n",
        "        positive_contrib = sigmoid_o[:, y]\n",
        "        negative_contrib = np.dot(sigmoid_o, p_y)\n",
        "\n",
        "        grad_c = positive_contrib - negative_contrib\n",
        "        grad_W = np.outer(grad_c, x)\n",
        "        grad_U = np.zeros_like(self.U)\n",
        "        grad_U[:, y] = positive_contrib\n",
        "        grad_U -= sigmoid_o * p_y[np.newaxis, :]\n",
        "\n",
        "        grad_d = np.zeros(self.n_classes)\n",
        "        grad_d[y] = 1\n",
        "        grad_d -= p_y\n",
        "\n",
        "        return {'W': grad_W, 'U': grad_U, 'c': grad_c, 'd': grad_d}\n",
        "\n",
        "    def generative_gradient(self, x, y, n_gibbs=1):\n",
        "        \"\"\"Compute generative gradient using Contrastive Divergence (CD-k).\"\"\"\n",
        "        # Positive phase\n",
        "        p_h_pos, h_pos = self.sample_hidden(x, y)\n",
        "\n",
        "        # Negative phase (CD-1)\n",
        "        x_neg, y_neg = x.copy(), y\n",
        "        for _ in range(n_gibbs):\n",
        "            # Sample y_neg from p(y|h_pos)\n",
        "            p_y_neg = self.softmax(self.d + np.dot(self.U.T, h_pos))\n",
        "            y_neg = np.random.choice(self.n_classes, p=p_y_neg)\n",
        "            # Sample x_neg from p(x|h_pos)\n",
        "            p_x_neg = self.sigmoid(self.b + np.dot(self.W.T, h_pos))\n",
        "            x_neg = np.random.binomial(1, p_x_neg)\n",
        "            # Sample h_neg from p(h|x_neg, y_neg)\n",
        "            _, h_neg = self.sample_hidden(x_neg, y_neg)\n",
        "\n",
        "        # Compute gradients\n",
        "        grad_W = np.outer(h_pos, x) - np.outer(h_neg, x_neg)\n",
        "        grad_U = np.zeros_like(self.U)\n",
        "        grad_U[:, y] = h_pos\n",
        "        grad_U[:, y_neg] -= h_neg\n",
        "        grad_b = x - x_neg\n",
        "        grad_c = h_pos - h_neg\n",
        "        grad_d = np.zeros(self.n_classes)\n",
        "        grad_d[y] = 1\n",
        "        grad_d[y_neg] -= 1\n",
        "\n",
        "        return {'W': grad_W, 'U': grad_U, 'b': grad_b, 'c': grad_c, 'd': grad_d}\n",
        "\n",
        "    def update_parameters(self, grads, lr):\n",
        "        \"\"\"Update parameters with gradients.\"\"\"\n",
        "        self.W += lr * grads.get('W', 0)\n",
        "        self.U += lr * grads.get('U', 0)\n",
        "        self.b += lr * grads.get('b', 0)\n",
        "        self.c += lr * grads.get('c', 0)\n",
        "        self.d += lr * grads.get('d', 0)\n",
        "\n",
        "    def train(self, labeled_data, lr=0.01, alpha=0.1, epochs=10):\n",
        "        \"\"\"Training loop for HDRBM.\"\"\"\n",
        "        for _ in range(epochs):\n",
        "            np.random.shuffle(labeled_data)\n",
        "            for x, y in labeled_data:\n",
        "                # Hybrid gradient: discriminative + alpha * generative\n",
        "                disc_grads = self.discriminative_gradient(x, y)\n",
        "                gen_grads = self.generative_gradient(x, y)\n",
        "                hybrid_grads = {k: disc_grads[k] + alpha * gen_grads.get(k, 0) for k in disc_grads}\n",
        "                self.update_parameters(hybrid_grads, lr)\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Binarize the data\n",
        "X_binarized = (X > 127).astype(int)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare labeled data as (input, target) pairs\n",
        "labeled_data = list(zip(X_train.values, y_train))\n",
        "\n",
        "# Initialize HDRBM\n",
        "n_visible = 784  # 28x28 pixels\n",
        "n_hidden = 200   # Number of hidden units\n",
        "n_classes = 10   # Digits 0-9\n",
        "hdrbm = HDRBM(n_visible, n_hidden, n_classes)\n",
        "\n",
        "# Train HDRBM with alpha=0.1\n",
        "hdrbm.train(labeled_data, lr=0.01, alpha=0.1, epochs=10)\n",
        "\n",
        "# Evaluate p(y|x) on the test set\n",
        "y_pred = []\n",
        "for x in X_test.values:\n",
        "    p_y = hdrbm.p_y_given_x(x)\n",
        "    y_pred.append(np.argmax(p_y))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499e813f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-02T15:31:13.511034Z",
          "start_time": "2025-02-02T15:07:08.993606Z"
        },
        "id": "499e813f",
        "outputId": "3e445ddb-efd1-482b-b7ba-895b815f291a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 94.04%\n"
          ]
        }
      ],
      "source": [
        "hdrbm.train(labeled_data, lr=0.01, alpha=1, epochs=10)\n",
        "\n",
        "# Evaluate p(y|x) on the test set\n",
        "y_pred = []\n",
        "for x in X_test.values:\n",
        "    p_y = hdrbm.p_y_given_x(x)\n",
        "    y_pred.append(np.argmax(p_y))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727d71f7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-02T15:57:25.967673Z",
          "start_time": "2025-02-02T15:31:13.513029Z"
        },
        "id": "727d71f7",
        "outputId": "93acd150-25dd-4244-bda6-5cfae268298e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 96.24%\n"
          ]
        }
      ],
      "source": [
        "hdrbm.train(labeled_data, lr=0.01, alpha=0.01, epochs=10)\n",
        "\n",
        "# Evaluate p(y|x) on the test set\n",
        "y_pred = []\n",
        "for x in X_test.values:\n",
        "    p_y = hdrbm.p_y_given_x(x)\n",
        "    y_pred.append(np.argmax(p_y))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "k5oSZIIXkgCB"
      },
      "id": "k5oSZIIXkgCB"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b0c92709",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0c92709",
        "outputId": "ac25a14e-0623-4db6-96bb-54599273f519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with fully supervised data (100% labelled):\n",
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 completed.\n",
            "Epoch 4 completed.\n",
            "Epoch 5 completed.\n",
            "Epoch 6 completed.\n",
            "Epoch 7 completed.\n",
            "Epoch 8 completed.\n",
            "Epoch 9 completed.\n",
            "Epoch 10 completed.\n",
            "Test Accuracy: 95.80%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98      1343\n",
            "           1       0.98      0.99      0.98      1600\n",
            "           2       0.95      0.96      0.95      1380\n",
            "           3       0.97      0.92      0.95      1433\n",
            "           4       0.96      0.95      0.96      1295\n",
            "           5       0.93      0.96      0.95      1273\n",
            "           6       0.98      0.96      0.97      1396\n",
            "           7       0.97      0.96      0.97      1503\n",
            "           8       0.94      0.94      0.94      1357\n",
            "           9       0.92      0.96      0.94      1420\n",
            "\n",
            "    accuracy                           0.96     14000\n",
            "   macro avg       0.96      0.96      0.96     14000\n",
            "weighted avg       0.96      0.96      0.96     14000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1309    1    7    0    2    5    3    2   10    4]\n",
            " [   0 1578    7    2    2    3    0    2    5    1]\n",
            " [   0    5 1323    8    8    3    7    8   14    4]\n",
            " [   3    4   27 1322    1   39    1    8   11   17]\n",
            " [   0    2    5    0 1234    1    5    3    6   39]\n",
            " [   2    1    1   13    1 1227    9    0   15    4]\n",
            " [   7    1   10    0   10   17 1341    1    9    0]\n",
            " [   2    8    7    0    3    5    0 1440    4   34]\n",
            " [   4   10    9   13    3   17    3    6 1275   17]\n",
            " [   4    4    3    5   18    6    0   10    7 1363]]\n",
            "\n",
            "Training with semi-supervised data (10% labelled):\n",
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 completed.\n",
            "Epoch 4 completed.\n",
            "Epoch 5 completed.\n",
            "Epoch 6 completed.\n",
            "Epoch 7 completed.\n",
            "Epoch 8 completed.\n",
            "Epoch 9 completed.\n",
            "Epoch 10 completed.\n",
            "Test Accuracy: 91.86%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.97      0.95      1343\n",
            "           1       0.94      0.98      0.96      1600\n",
            "           2       0.94      0.89      0.92      1380\n",
            "           3       0.91      0.91      0.91      1433\n",
            "           4       0.92      0.92      0.92      1295\n",
            "           5       0.94      0.79      0.86      1273\n",
            "           6       0.93      0.96      0.94      1396\n",
            "           7       0.95      0.94      0.94      1503\n",
            "           8       0.83      0.92      0.87      1357\n",
            "           9       0.90      0.89      0.90      1420\n",
            "\n",
            "    accuracy                           0.92     14000\n",
            "   macro avg       0.92      0.92      0.92     14000\n",
            "weighted avg       0.92      0.92      0.92     14000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1309    0    0    4    3    3    7    2   14    1]\n",
            " [   0 1561    8    4    1    4    1    2   17    2]\n",
            " [  24   11 1234    9   15    1   29   15   39    3]\n",
            " [   9    6   22 1297    3   20   14   16   35   11]\n",
            " [   4    3    7    1 1187    0   12    2   10   69]\n",
            " [  28   11    7   71   12 1005   36    2   90   11]\n",
            " [   7    5    7    0    9   14 1346    1    7    0]\n",
            " [  12   16   10    1   12    1    0 1411    9   31]\n",
            " [   8   30   12   18   11   12   10    4 1246    6]\n",
            " [  11    9    7   14   41    5    0   37   31 1265]]\n",
            "\n",
            "Training with highly semi-supervised data (1% labelled):\n",
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 completed.\n",
            "Epoch 4 completed.\n",
            "Epoch 5 completed.\n",
            "Epoch 6 completed.\n",
            "Epoch 7 completed.\n",
            "Epoch 8 completed.\n",
            "Epoch 9 completed.\n",
            "Epoch 10 completed.\n",
            "Test Accuracy: 82.24%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94      1343\n",
            "           1       0.91      0.92      0.92      1600\n",
            "           2       0.87      0.85      0.86      1380\n",
            "           3       0.70      0.85      0.77      1433\n",
            "           4       0.95      0.74      0.83      1295\n",
            "           5       0.95      0.31      0.47      1273\n",
            "           6       0.91      0.94      0.92      1396\n",
            "           7       0.96      0.81      0.88      1503\n",
            "           8       0.58      0.91      0.71      1357\n",
            "           9       0.75      0.89      0.81      1420\n",
            "\n",
            "    accuracy                           0.82     14000\n",
            "   macro avg       0.85      0.82      0.81     14000\n",
            "weighted avg       0.85      0.82      0.82     14000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1282    1    4    2    0    1   22    3   24    4]\n",
            " [   0 1476   12    7    1    0    3    2   95    4]\n",
            " [  14   28 1167   31    9    0   37   14   69   11]\n",
            " [  16   11   58 1221    0   10   13   10   78   16]\n",
            " [   1    4   16   13  953    0   18    0   80  210]\n",
            " [  20   29    7  371    5  400   25    1  402   13]\n",
            " [  10    7   15    3    5    6 1313    0   37    0]\n",
            " [   6   38   42   20    8    0    2 1212   33  142]\n",
            " [  22    8   17   39    5    3   10    4 1231   18]\n",
            " [  11   17   10   25   12    0    1   18   67 1259]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Define the Semi-Supervised DRBM model\n",
        "class SemiSupervisedDRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_classes):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Initialise parameters\n",
        "        self.W = np.random.randn(n_hidden, n_visible) * 0.01  # Weights (hidden x visible)\n",
        "        self.U = np.random.randn(n_hidden, n_classes) * 0.01    # Weights (hidden x class)\n",
        "        self.b = np.zeros(n_visible)  # Visible bias\n",
        "        self.c = np.zeros(n_hidden)   # Hidden bias\n",
        "        self.d = np.zeros(n_classes)  # Class bias\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Softmax applied column-wise\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def p_y_given_x(self, x):\n",
        "        \"\"\"\n",
        "        Compute p(y|x) using exact inference.\n",
        "        x: a 1D numpy array of length n_visible.\n",
        "        Returns a probability vector of length n_classes.\n",
        "        \"\"\"\n",
        "        # Compute activation for each class\n",
        "        # For each hidden unit j, activation_j = c_j + sum_i W[j, i]*x_i and add U[j, y] per class\n",
        "        activation = self.c[:, np.newaxis] + np.dot(self.W, x)[:, np.newaxis] + self.U\n",
        "        # Compute free energy contribution from hidden units\n",
        "        log_terms = np.log1p(np.exp(activation))  # log(1 + exp(.))\n",
        "        sum_log_terms = np.sum(log_terms, axis=0)  # sum over hidden units\n",
        "        logits = self.d + sum_log_terms\n",
        "        return self.softmax(logits)\n",
        "\n",
        "    def sample_hidden(self, x, y):\n",
        "        \"\"\"\n",
        "        Sample hidden units given x and y.\n",
        "        Returns the hidden probabilities and a binary sample.\n",
        "        \"\"\"\n",
        "        activation = self.c + np.dot(self.W, x) + self.U[:, y]\n",
        "        p_h = self.sigmoid(activation)\n",
        "        h_sample = np.random.binomial(1, p_h)\n",
        "        return p_h, h_sample\n",
        "\n",
        "    def generative_gradient(self, x, y, n_gibbs=1):\n",
        "        \"\"\"\n",
        "        Compute generative gradient using Contrastive Divergence (CD-k).\n",
        "        This function is similar to the one used in HDRBM.\n",
        "        \"\"\"\n",
        "        # Positive phase\n",
        "        p_h_pos, h_pos = self.sample_hidden(x, y)\n",
        "\n",
        "        # Negative phase (CD-1)\n",
        "        x_neg, y_neg = x.copy(), y\n",
        "        for _ in range(n_gibbs):\n",
        "            # Sample y_neg from p(y|h_pos)\n",
        "            p_y_neg = self.softmax(self.d + np.dot(self.U.T, h_pos))\n",
        "            y_neg = np.random.choice(self.n_classes, p=p_y_neg)\n",
        "            # Sample x_neg from p(x|h_pos)\n",
        "            p_x_neg = self.sigmoid(self.b + np.dot(self.W.T, h_pos))\n",
        "            x_neg = np.random.binomial(1, p_x_neg)\n",
        "            # Sample h_neg from p(h|x_neg, y_neg)\n",
        "            _, h_neg = self.sample_hidden(x_neg, y_neg)\n",
        "\n",
        "        # Compute gradients (similar to HDRBM)\n",
        "        grad_W = np.outer(h_pos, x) - np.outer(h_neg, x_neg)\n",
        "        grad_U = np.zeros_like(self.U)\n",
        "        grad_U[:, y] = h_pos\n",
        "        grad_U[:, y_neg] -= h_neg\n",
        "        grad_b = x - x_neg\n",
        "        grad_c = h_pos - h_neg\n",
        "        grad_d = np.zeros(self.n_classes)\n",
        "        grad_d[y] = 1\n",
        "        grad_d[y_neg] -= 1\n",
        "\n",
        "        return {'W': grad_W, 'U': grad_U, 'b': grad_b, 'c': grad_c, 'd': grad_d}\n",
        "\n",
        "    def discriminative_gradient(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the exact gradient for the discriminative loss: -log p(y|x).\n",
        "        This is based on the DRBM implementation.\n",
        "        \"\"\"\n",
        "        p_y = self.p_y_given_x(x)\n",
        "        # Compute the activation matrix for the hidden layer\n",
        "        activation = self.c[:, np.newaxis] + np.dot(self.W, x)[:, np.newaxis] + self.U\n",
        "        sigmoid_o = self.sigmoid(activation)\n",
        "\n",
        "        # Gradients for c, W, U, d\n",
        "        positive_contrib = sigmoid_o[:, y]\n",
        "        negative_contrib = np.dot(sigmoid_o, p_y)\n",
        "\n",
        "        grad_c = positive_contrib - negative_contrib\n",
        "        grad_W = np.outer(grad_c, x)\n",
        "        grad_U = np.zeros_like(self.U)\n",
        "        grad_U[:, y] = positive_contrib\n",
        "        grad_U -= sigmoid_o * p_y[np.newaxis, :]\n",
        "\n",
        "        grad_d = np.zeros(self.n_classes)\n",
        "        grad_d[y] = 1\n",
        "        grad_d -= p_y\n",
        "\n",
        "        return {'W': grad_W, 'U': grad_U, 'c': grad_c, 'd': grad_d}\n",
        "\n",
        "    def update_parameters(self, grads, lr):\n",
        "        \"\"\"Update model parameters using gradient descent.\"\"\"\n",
        "        self.W += lr * grads.get('W', 0)\n",
        "        self.U += lr * grads.get('U', 0)\n",
        "        self.b += lr * grads.get('b', 0)\n",
        "        self.c += lr * grads.get('c', 0)\n",
        "        self.d += lr * grads.get('d', 0)\n",
        "\n",
        "    def train(self, labelled_data, unlabelled_data, lr=0.01, alpha=0.1, epochs=10):\n",
        "        \"\"\"\n",
        "        Train the model with both labelled and unlabelled data.\n",
        "\n",
        "        labelled_data: list of tuples (x, y) where x is a numpy array and y is the true label.\n",
        "        unlabelled_data: list of x samples (unlabelled, but we have their x values).\n",
        "        \"\"\"\n",
        "        # Combine labelled and unlabelled data (we treat them separately in the loop)\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            np.random.shuffle(labelled_data)\n",
        "            np.random.shuffle(unlabelled_data)\n",
        "\n",
        "            # Process labelled samples\n",
        "            for x, y in labelled_data:\n",
        "                disc_grads = self.discriminative_gradient(x, y)\n",
        "                gen_grads = self.generative_gradient(x, y)\n",
        "                # Hybrid gradient for labelled sample\n",
        "                hybrid_grads = {k: disc_grads.get(k, 0) + alpha * gen_grads.get(k, 0) for k in disc_grads}\n",
        "                self.update_parameters(hybrid_grads, lr)\n",
        "\n",
        "            # Process unlabelled samples using pseudo-labels\n",
        "            for x in unlabelled_data:\n",
        "                # Obtain pseudo-label from current model\n",
        "                p_y = self.p_y_given_x(x)\n",
        "                pseudo_y = np.argmax(p_y)\n",
        "                gen_grads = self.generative_gradient(x, pseudo_y)\n",
        "                # For unlabelled data we only use the generative component\n",
        "                grad = {k: alpha * gen_grads.get(k, 0) for k in gen_grads}\n",
        "                self.update_parameters(grad, lr)\n",
        "\n",
        "            print(f\"Epoch {epoch} completed.\")\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Predict the label for a given input sample.\n",
        "        \"\"\"\n",
        "        p_y = self.p_y_given_x(x)\n",
        "        return np.argmax(p_y)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test set and print the accuracy, classification report and confusion matrix.\n",
        "        \"\"\"\n",
        "        y_pred = []\n",
        "        for x in X_test:\n",
        "            y_pred.append(self.predict(x))\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# -----------------------\n",
        "# Data Preparation\n",
        "# -----------------------\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Binarise the data\n",
        "X_binarised = (X > 127).astype(int)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binarised, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------\n",
        "# Create labelled and unlabelled sets\n",
        "# -----------------------\n",
        "def create_semisupervised_sets(X, y, labelled_fraction=1.0):\n",
        "    \"\"\"\n",
        "    Split the training data into labelled and unlabelled sets.\n",
        "    labelled_fraction: fraction of the training samples to be used as labelled data.\n",
        "    \"\"\"\n",
        "    X_np = X.to_numpy()  # convert to NumPy array for positional indexing\n",
        "    y_np = y.to_numpy()  # similarly for y\n",
        "    n_samples = X_np.shape[0]\n",
        "    n_labelled = int(n_samples * labelled_fraction)\n",
        "\n",
        "    # Shuffle the indices\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    labelled_idx = indices[:n_labelled]\n",
        "    unlabelled_idx = indices[n_labelled:]\n",
        "\n",
        "    labelled_data = [(X_np[i].astype(np.float32), y_np[i]) for i in labelled_idx]\n",
        "    unlabelled_data = [X_np[i].astype(np.float32) for i in unlabelled_idx]\n",
        "\n",
        "    return labelled_data, unlabelled_data\n",
        "\n",
        "\n",
        "# For fully supervised training (100% labelled)\n",
        "labelled_full, unlabelled_empty = create_semisupervised_sets(X_train, y_train, labelled_fraction=1.0)\n",
        "# For semi-supervised training (10% labelled)\n",
        "labelled_10, unlabelled_10 = create_semisupervised_sets(X_train, y_train, labelled_fraction=0.1)\n",
        "# For highly semi-supervised training (1% labelled)\n",
        "labelled_1, unlabelled_1 = create_semisupervised_sets(X_train, y_train, labelled_fraction=0.01)\n",
        "\n",
        "# -----------------------\n",
        "# Initialise and Train the Model\n",
        "# -----------------------\n",
        "n_visible = 784   # 28x28 pixels\n",
        "n_hidden = 200    # Number of hidden units\n",
        "n_classes = 10    # Digits 0-9\n",
        "\n",
        "print(\"Training with fully supervised data (100% labelled):\")\n",
        "drbm_full = SemiSupervisedDRBM(n_visible, n_hidden, n_classes)\n",
        "drbm_full.train(labelled_full, unlabelled_empty, lr=0.01, alpha=0.1, epochs=10)\n",
        "X_test_array = X_test.values.astype(np.float32)\n",
        "drbm_full.evaluate(X_test_array, y_test)\n",
        "\n",
        "\n",
        "print(\"\\nTraining with semi-supervised data (10% labelled):\")\n",
        "drbm_10 = SemiSupervisedDRBM(n_visible, n_hidden, n_classes)\n",
        "drbm_10.train(labelled_10, unlabelled_10, lr=0.01, alpha=0.1, epochs=10)\n",
        "# Prepare X_test in the same way\n",
        "X_test_array = [x.values.astype(np.float32) for i, x in X_test.iterrows()]\n",
        "drbm_10.evaluate(X_test_array, y_test)\n",
        "\n",
        "print(\"\\nTraining with highly semi-supervised data (1% labelled):\")\n",
        "drbm_1 = SemiSupervisedDRBM(n_visible, n_hidden, n_classes)\n",
        "drbm_1.train(labelled_1, unlabelled_1, lr=0.01, alpha=0.1, epochs=10)\n",
        "drbm_1.evaluate(X_test_array, y_test)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}